{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple RL\n",
    "====================\n",
    "\n",
    "Welcome! Here we'll showcase some basic examples of typical RL programming tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Grid World\n",
    "----------\n",
    "\n",
    "First, we'll grab our relevant imports: some agents, an MDP, an a function to facilitate running experiments and plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banerjs/Workspaces/baselines/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add simple_rl to system path.\n",
    "import os\n",
    "import sys\n",
    "# parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "# sys.path.insert(0, parent_dir)\n",
    "\n",
    "from simple_rl.agents import QLearningAgent, RandomAgent\n",
    "from simple_rl.tasks import GridWorldMDP\n",
    "from simple_rl.run_experiments import run_agents_on_mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make an MDP and a few agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup MDP.\n",
    "mdp = GridWorldMDP(width=6, height=6, init_loc=(1,1), goal_locs=[(6,6)])\n",
    "\n",
    "# Setup Agents.\n",
    "ql_agent = QLearningAgent(actions=mdp.get_actions()) \n",
    "rand_agent = RandomAgent(actions=mdp.get_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real meat of <i>simple_rl</i> are the functions that run experiments. The first of which takes a list of agents and an mdp and simulates their interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: \n",
      "(MDP)\n",
      "\tgridworld_h-6_w-6\n",
      "(Agents)\n",
      "\tQ-learning,0\n",
      "\tRandom,1\n",
      "(Params)\n",
      "\tinstances : 5\n",
      "\tepisodes : 100\n",
      "\tsteps : 40\n",
      "\tgamma : 0.99\n",
      "\ttrack_disc_reward : False\n",
      "\tis_lifelong : False\n",
      "\n",
      "Q-learning is learning.\n",
      "  Instance 1 of 5.\n",
      "  Instance 2 of 5.\n",
      "  Instance 3 of 5.\n",
      "  Instance 4 of 5.\n",
      "  Instance 5 of 5.\n",
      "\n",
      "Random is learning.\n",
      "  Instance 1 of 5.\n",
      "  Instance 2 of 5.\n",
      "  Instance 3 of 5.\n",
      "  Instance 4 of 5.\n",
      "  Instance 5 of 5.\n",
      "\n",
      "\n",
      "--- TIMES ---\n",
      "Q-learning agent took 1.04 seconds.\n",
      "Random agent took 0.34 seconds.\n",
      "-------------\n",
      "\n",
      "\tQ-learning: 183.6 (conf_interv: 27.77 )\n",
      "\tRandom: 10.4 (conf_interv: 1.89 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run experiment and make plot.\n",
    "run_agents_on_mdp([ql_agent, rand_agent], mdp, instances=5, episodes=100, steps=40, reset_at_terminal=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can throw R-Max, introduced by [[Brafman and Tennenholtz, 2002]](http://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf) in the mix, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: \n",
      "(MDP)\n",
      "\tgridworld_h-6_w-6\n",
      "(Agents)\n",
      "\tRMax-h3,0\n",
      "\tQ-learning,1\n",
      "\tRandom,2\n",
      "(Params)\n",
      "\tinstances : 5\n",
      "\tepisodes : 100\n",
      "\tsteps : 20\n",
      "\tgamma : 0.99\n",
      "\ttrack_disc_reward : False\n",
      "\tis_lifelong : False\n",
      "\n",
      "RMax-h3 is learning.\n",
      "  Instance 1 of 5.\n",
      "  Instance 2 of 5.\n",
      "  Instance 3 of 5.\n",
      "  Instance 4 of 5.\n",
      "  Instance 5 of 5.\n",
      "\n",
      "Q-learning is learning.\n",
      "  Instance 1 of 5.\n",
      "  Instance 2 of 5.\n",
      "  Instance 3 of 5.\n",
      "  Instance 4 of 5.\n",
      "  Instance 5 of 5.\n",
      "\n",
      "Random is learning.\n",
      "  Instance 1 of 5.\n",
      "  Instance 2 of 5.\n",
      "  Instance 3 of 5.\n",
      "  Instance 4 of 5.\n",
      "  Instance 5 of 5.\n",
      "\n",
      "\n",
      "--- TIMES ---\n",
      "RMax-h3 agent took 34.87 seconds.\n",
      "Q-learning agent took 0.53 seconds.\n",
      "Random agent took 0.22 seconds.\n",
      "-------------\n",
      "\n",
      "\tRMax-h3: 27.4 (conf_interv: 2.86 )\n",
      "\tQ-learning: 7.4 (conf_interv: 10.86 )\n",
      "\tRandom: 2.2 (conf_interv: 1.02 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from simple_rl.agents import RMaxAgent\n",
    "\n",
    "rmax_agent = RMaxAgent(actions=mdp.get_actions(), horizon=3, s_a_threshold=1)\n",
    "\n",
    "# Run experiment and make plot.\n",
    "run_agents_on_mdp([rmax_agent, ql_agent, rand_agent], mdp, instances=5, episodes=100, steps=20, reset_at_terminal=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each experiment we run generates an Experiment object. This facilitates recording results, making relevant files, and plotting. When the <code>run_agents...</code> function is called, a <i>results</i> dir is created containing relevant experiment data. There should be a subdirectory in <i>results</i> named after the mdp you ran experiments on -- this is where the plot, agent results, and <i>parameters.txt</i> file are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above code is contained in the [<i>simple_example.py</i>](https://github.com/david-abel/simple_rl/blob/master/examples/simple_example.py) file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Visuals (require pygame)\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's make a FourRoomMDP from [[Sutton, Precup, Singh 1999]](http://ac.els-cdn.com/S0004370299000521/1-s2.0-S0004370299000521-main.pdf?_tid=e985f9ca-7b76-11e7-949a-00000aab0f26&acdnat=1502113811_cb6bca11a9512136fb3f55155abb4146), which is more visually interesting than a grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press anything to quit \n"
     ]
    }
   ],
   "source": [
    "from simple_rl.tasks import FourRoomMDP\n",
    "four_room_mdp = FourRoomMDP(9, 9, goal_locs=[(9, 9)], gamma=0.95)\n",
    "\n",
    "# Run experiment and make plot.\n",
    "four_room_mdp.visualize_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"val.png\" alt=\"Val\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can visualize a policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pol.png\" alt=\"Val Visual\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these are in examples/viz_example.py. If you need pygame in anaconda, give this a shot:\n",
    "\n",
    "    > conda install -c cogsci pygame\n",
    "\n",
    "If you get an sdl font related error on Mac/Linux, try:\n",
    "\n",
    "    > brew update sdl && sdl_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make grid worlds with a text file. For instance, we can construct the grid problem from [[Barto and Pickett 2002]](https://www.researchgate.net/profile/Marc_Pickett/publication/2534967_PolicyBlocks_An_Algorithm_for_Creating_Useful_Macro-Actions_in_Reinforcement_Learning/links/0c960536b7efb97cd0000000.pdf) by making a text file:\n",
    "\n",
    "\n",
    "    --w-----w---w----g\n",
    "    --------w---------\n",
    "    --w-----w---w-----\n",
    "    --w-----w---w-----\n",
    "    wwwww-wwwwwwwww-ww\n",
    "    ---w----w----w----\n",
    "    ---w---------w----\n",
    "    --------w---------\n",
    "    wwwwwwwww---------\n",
    "    w-------wwwwwww-ww\n",
    "    --w-----w---w-----\n",
    "    --------w---------\n",
    "    --w---------w-----\n",
    "    --w-----w---w-----\n",
    "    wwwww-wwwwwwwww-ww\n",
    "    ---w-----w---w----\n",
    "    ---w-----w---w----\n",
    "    a--------w--------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we make a grid world out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press anything to quit \n"
     ]
    }
   ],
   "source": [
    "from simple_rl.tasks.grid_world import GridWorldMDPClass\n",
    "\n",
    "pblocks_mdp = GridWorldMDPClass.make_grid_world_from_file(\"pblocks_grid.txt\", randomize=False)\n",
    "pblocks_mdp.visualize_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which Produces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pblocks.png\" alt=\"Policy Blocks Grid World\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Example 3: OOMDPs, Taxi\n",
    "---------------\n",
    "\n",
    "There's also a Taxi MDP, which is actually built on top of an Object Oriented MDP Abstract class from [[Diuk, Cohen, Littman 2008]](https://carlosdiuk.github.io/papers/OORL.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_rl.tasks import TaxiOOMDP\n",
    "from simple_rl.run_experiments import run_agents_on_mdp\n",
    "from simple_rl.agents import QLearningAgent, RandomAgent\n",
    "\n",
    "# Taxi initial state attributes..\n",
    "agent = {\"x\":1, \"y\":1, \"has_passenger\":0}\n",
    "passengers = [{\"x\":3, \"y\":2, \"dest_x\":2, \"dest_y\":3, \"in_taxi\":0}]\n",
    "taxi_mdp = TaxiOOMDP(width=4, height=4, agent=agent, walls=[], passengers=passengers)\n",
    "\n",
    "# Make agents.\n",
    "ql_agent = QLearningAgent(actions=taxi_mdp.get_actions()) \n",
    "rand_agent = RandomAgent(actions=taxi_mdp.get_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we specify the objects of the OOMDP and their attributes. Now, just as before, we can let some agents interact with the MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: \n",
      "(MDP)\n",
      "\ttaxi_h-4_w-4\n",
      "(Agents)\n",
      "\tQ-learning,0\n",
      "\tRandom,1\n",
      "(Params)\n",
      "\tinstances : 5\n",
      "\tepisodes : 100\n",
      "\tsteps : 150\n",
      "\tgamma : 0.99\n",
      "\ttrack_disc_reward : False\n",
      "\tis_lifelong : False\n",
      "\n",
      "Q-learning is learning.\n",
      "  Instance 1 of 5.\n",
      "  Instance 2 of 5.\n",
      "  Instance 3 of 5.\n",
      "  Instance 4 of 5.\n",
      "  Instance 5 of 5.\n",
      "\n",
      "Random is learning.\n",
      "  Instance 1 of 5.\n",
      "  Instance 2 of 5.\n",
      "  Instance 3 of 5.\n",
      "  Instance 4 of 5.\n",
      "  Instance 5 of 5.\n",
      "\n",
      "\n",
      "--- TIMES ---\n",
      "Q-learning agent took 41.17 seconds.\n",
      "Random agent took 11.6 seconds.\n",
      "-------------\n",
      "\n",
      "\tQ-learning: 1262.8 (conf_interv: 143.11 )\n",
      "\tRandom: 5.6 (conf_interv: 1.42 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run experiment and make plot.\n",
    "run_agents_on_mdp([ql_agent, rand_agent], taxi_mdp, instances=5, episodes=100, steps=150, reset_at_terminal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on OOMDPs in [<i>examples/oomdp_example.py</i>](https://github.com/david-abel/simple_rl/blob/master/examples/oomdp_example.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4: Markov Games\n",
    "    --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've added a few markov games, including rock paper scissors, grid games, and prisoners dilemma. Just as before, we get a run agents method that simulates learning and makes a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: \n",
      "(Markov Game MDP)\n",
      "\trock_paper_scissors\n",
      "(Agents)\n",
      "\tQ-learning,0\n",
      "\tfixed-policy,1\n",
      "(Params)\n",
      "\tinstances : 10\n",
      "\ttrack_disc_reward : False\n",
      "\tis_lifelong : False\n",
      "\n",
      "\tInstance 1 of 10.\n",
      "\tInstance 2 of 10.\n",
      "\tInstance 3 of 10.\n",
      "\tInstance 4 of 10.\n",
      "\tInstance 5 of 10.\n",
      "\tInstance 6 of 10.\n",
      "\tInstance 7 of 10.\n",
      "\tInstance 8 of 10.\n",
      "\tInstance 9 of 10.\n",
      "\tInstance 10 of 10.\n",
      "Experiment took 0.03 seconds.\n",
      "\tQ-learning: 7.3 (conf_interv: 1.3 )\n",
      "\tfixed-policy: -7.3 (conf_interv: 1.3 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from simple_rl.run_experiments import play_markov_game\n",
    "from simple_rl.agents import QLearningAgent, FixedPolicyAgent\n",
    "from simple_rl.tasks import RockPaperScissorsMDP\n",
    "\n",
    "import random\n",
    "\n",
    "# Setup MDP, Agents.\n",
    "markov_game = RockPaperScissorsMDP()\n",
    "ql_agent = QLearningAgent(actions=markov_game.get_actions(), epsilon=0.2) \n",
    "fixed_action = random.choice(markov_game.get_actions())\n",
    "fixed_agent = FixedPolicyAgent(policy=lambda s:fixed_action)\n",
    "\n",
    "# Run experiment and make plot.\n",
    "play_markov_game([ql_agent, fixed_agent], markov_game, instances=10, episodes=1, steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5: Gym MDP\n",
    "    --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently I added support for making OpenAI gym MDPs. It's again only a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Running experiment: \n",
      "(MDP)\n",
      "\tgym-CartPole-v0\n",
      "(Agents)\n",
      "\tLinear-Q-rbf,0\n",
      "(Params)\n",
      "\tinstances : 3\n",
      "\tepisodes : 1\n",
      "\tsteps : 50\n",
      "\tgamma : 0.99\n",
      "\ttrack_disc_reward : False\n",
      "\tis_lifelong : False\n",
      "\n",
      "Linear-Q-rbf is learning.\n",
      "  Instance 1 of 3.\n",
      "  Instance 2 of 3.\n",
      "  Instance 3 of 3.\n",
      "\n",
      "\n",
      "--- TIMES ---\n",
      "Linear-Q-rbf agent took 0.07 seconds.\n",
      "-------------\n",
      "\n",
      "\tLinear-Q-rbf: 18.66667 (conf_interv: 12.27 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from simple_rl.tasks import GymMDP\n",
    "from simple_rl.agents import LinearQAgent, RandomAgent\n",
    "from simple_rl.run_experiments import run_agents_on_mdp\n",
    "\n",
    "# Gym MDP.\n",
    "gym_mdp = GymMDP(env_name='CartPole-v0', render=False) # If render is true, visualizes interactions.\n",
    "num_feats = gym_mdp.get_num_state_feats()\n",
    "\n",
    "# Setup agents and run.\n",
    "lin_agent = LinearQAgent(gym_mdp.get_actions(), num_features=num_feats, alpha=0.2, epsilon=0.4, rbf=True)\n",
    "\n",
    "run_agents_on_mdp([lin_agent], gym_mdp, instances=3, episodes=1, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
